{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO6JWXOTjl9_"
      },
      "source": [
        "### Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGN7rZKFjxxV",
        "outputId": "a7b52477-9900-410e-c7f3-812f1627aae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.6)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (4.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.10.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libsndfile1 is already the newest version (1.0.28-4ubuntu0.18.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install numpy scipy librosa unidecode inflect librosa\n",
        "apt-get update\n",
        "apt-get install -y libsndfile1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dHsq68dkjl-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlmAK9jR4kaC",
        "outputId": "cd1144ec-7dad-481e-8145-d36691c2eedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unzipping Dataset from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NiJSItTdFXU",
        "outputId": "f0a0e7d9-55c4-4387-f583-b46417ef5b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/My Drive/DL_project/dataset/lec_01_audio_segmented.zip\n",
            "replace lec_01_audio_segmented/seg_1.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip gdrive/My\\ Drive/DL_project/dataset/lec_01_audio_segmented.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKwsK--hVtK_",
        "outputId": "cf4fb58c-30c3-415e-f899-0c2741fbfdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/My Drive/DL_project/dataset/lec_01_segmented_frames.zip\n",
            "replace lec_01_segmented_frames/seg_1/frame_0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip gdrive/My\\ Drive/DL_project/dataset/lec_01_segmented_frames.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjHCwgTpjl-b"
      },
      "source": [
        "##### Device Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENpBLfrQjl-c",
        "outputId": "95eabb7c-25e9-4872-9cd0-20dae222252e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQsx9N6jjl-K"
      },
      "source": [
        "### Defining Different Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epzfVjN9knfe"
      },
      "source": [
        "### Load pretrained Tacotron2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSCdYm7oj1Vr",
        "outputId": "886e5dcf-c65a-481a-d7ed-caa3c4cbd474"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:14: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  \"pytorch_quantization module not found, quantization will not be available\"\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:18: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  \"pytorch_quantization module not found, quantization will not be available\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Tacotron2(\n",
              "  (embedding): Embedding(148, 512)\n",
              "  (encoder): Encoder(\n",
              "    (convolutions): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (prenet): Prenet(\n",
              "      (layers): ModuleList(\n",
              "        (0): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
              "        )\n",
              "        (1): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (attention_rnn): LSTMCell(768, 1024)\n",
              "    (attention_layer): Attention(\n",
              "      (query_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
              "      )\n",
              "      (memory_layer): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
              "      )\n",
              "      (v): LinearNorm(\n",
              "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
              "      )\n",
              "      (location_layer): LocationLayer(\n",
              "        (location_conv): ConvNorm(\n",
              "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
              "        )\n",
              "        (location_dense): LinearNorm(\n",
              "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\n",
              "    (linear_projection): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)\n",
              "    )\n",
              "    (gate_layer): LinearNorm(\n",
              "      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (postnet): Postnet(\n",
              "    (convolutions): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): ConvNorm(\n",
              "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "        )\n",
              "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
        "tacotron2 = tacotron2.to(device)\n",
        "tacotron2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJfsVDsOj65n",
        "outputId": "b955c8a0-29f3-4488-d8cd-ed31082bce99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['embedding.weight', 'encoder.convolutions.0.0.conv.weight', 'encoder.convolutions.0.0.conv.bias', 'encoder.convolutions.0.1.weight', 'encoder.convolutions.0.1.bias', 'encoder.convolutions.0.1.running_mean', 'encoder.convolutions.0.1.running_var', 'encoder.convolutions.0.1.num_batches_tracked', 'encoder.convolutions.1.0.conv.weight', 'encoder.convolutions.1.0.conv.bias', 'encoder.convolutions.1.1.weight', 'encoder.convolutions.1.1.bias', 'encoder.convolutions.1.1.running_mean', 'encoder.convolutions.1.1.running_var', 'encoder.convolutions.1.1.num_batches_tracked', 'encoder.convolutions.2.0.conv.weight', 'encoder.convolutions.2.0.conv.bias', 'encoder.convolutions.2.1.weight', 'encoder.convolutions.2.1.bias', 'encoder.convolutions.2.1.running_mean', 'encoder.convolutions.2.1.running_var', 'encoder.convolutions.2.1.num_batches_tracked', 'encoder.lstm.weight_ih_l0', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'decoder.prenet.layers.0.linear_layer.weight', 'decoder.prenet.layers.1.linear_layer.weight', 'decoder.attention_rnn.weight_ih', 'decoder.attention_rnn.weight_hh', 'decoder.attention_rnn.bias_ih', 'decoder.attention_rnn.bias_hh', 'decoder.attention_layer.query_layer.linear_layer.weight', 'decoder.attention_layer.memory_layer.linear_layer.weight', 'decoder.attention_layer.v.linear_layer.weight', 'decoder.attention_layer.location_layer.location_conv.conv.weight', 'decoder.attention_layer.location_layer.location_dense.linear_layer.weight', 'decoder.decoder_rnn.weight_ih', 'decoder.decoder_rnn.weight_hh', 'decoder.decoder_rnn.bias_ih', 'decoder.decoder_rnn.bias_hh', 'decoder.linear_projection.linear_layer.weight', 'decoder.linear_projection.linear_layer.bias', 'decoder.gate_layer.linear_layer.weight', 'decoder.gate_layer.linear_layer.bias', 'postnet.convolutions.0.0.conv.weight', 'postnet.convolutions.0.0.conv.bias', 'postnet.convolutions.0.1.weight', 'postnet.convolutions.0.1.bias', 'postnet.convolutions.0.1.running_mean', 'postnet.convolutions.0.1.running_var', 'postnet.convolutions.0.1.num_batches_tracked', 'postnet.convolutions.1.0.conv.weight', 'postnet.convolutions.1.0.conv.bias', 'postnet.convolutions.1.1.weight', 'postnet.convolutions.1.1.bias', 'postnet.convolutions.1.1.running_mean', 'postnet.convolutions.1.1.running_var', 'postnet.convolutions.1.1.num_batches_tracked', 'postnet.convolutions.2.0.conv.weight', 'postnet.convolutions.2.0.conv.bias', 'postnet.convolutions.2.1.weight', 'postnet.convolutions.2.1.bias', 'postnet.convolutions.2.1.running_mean', 'postnet.convolutions.2.1.running_var', 'postnet.convolutions.2.1.num_batches_tracked', 'postnet.convolutions.3.0.conv.weight', 'postnet.convolutions.3.0.conv.bias', 'postnet.convolutions.3.1.weight', 'postnet.convolutions.3.1.bias', 'postnet.convolutions.3.1.running_mean', 'postnet.convolutions.3.1.running_var', 'postnet.convolutions.3.1.num_batches_tracked', 'postnet.convolutions.4.0.conv.weight', 'postnet.convolutions.4.0.conv.bias', 'postnet.convolutions.4.1.weight', 'postnet.convolutions.4.1.bias', 'postnet.convolutions.4.1.running_mean', 'postnet.convolutions.4.1.running_var', 'postnet.convolutions.4.1.num_batches_tracked'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = tacotron2.state_dict()\n",
        "params.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ctMg4Fnoj-bS"
      },
      "outputs": [],
      "source": [
        "tacotron2.embedding = nn.Conv1d(148, 512, kernel_size=(3,), stride=(1,), padding=(2,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MzWWe9TokAmH"
      },
      "outputs": [],
      "source": [
        "for name, param in tacotron2.named_parameters():\n",
        "    if param.requires_grad and 'encoder' in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "      param.requires_grad = False\n",
        "tacotron2.embedding.weight.requires_grad = True "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv1nlZTXkUu8",
        "outputId": "af0fd924-8b25-407c-8d28-46523bb9b3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name : embedding.weight, parameter: torch.Size([512, 148, 3])\n",
            "name : encoder.convolutions.0.0.conv.weight, parameter: torch.Size([512, 512, 5])\n",
            "name : encoder.convolutions.0.0.conv.bias, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.0.1.weight, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.0.1.bias, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.1.0.conv.weight, parameter: torch.Size([512, 512, 5])\n",
            "name : encoder.convolutions.1.0.conv.bias, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.1.1.weight, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.1.1.bias, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.2.0.conv.weight, parameter: torch.Size([512, 512, 5])\n",
            "name : encoder.convolutions.2.0.conv.bias, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.2.1.weight, parameter: torch.Size([512])\n",
            "name : encoder.convolutions.2.1.bias, parameter: torch.Size([512])\n",
            "name : encoder.lstm.weight_ih_l0, parameter: torch.Size([1024, 512])\n",
            "name : encoder.lstm.weight_hh_l0, parameter: torch.Size([1024, 256])\n",
            "name : encoder.lstm.bias_ih_l0, parameter: torch.Size([1024])\n",
            "name : encoder.lstm.bias_hh_l0, parameter: torch.Size([1024])\n",
            "name : encoder.lstm.weight_ih_l0_reverse, parameter: torch.Size([1024, 512])\n",
            "name : encoder.lstm.weight_hh_l0_reverse, parameter: torch.Size([1024, 256])\n",
            "name : encoder.lstm.bias_ih_l0_reverse, parameter: torch.Size([1024])\n",
            "name : encoder.lstm.bias_hh_l0_reverse, parameter: torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "for name, param in tacotron2.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f\"name : {name}, parameter: {param.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HMxN0xtARfs0"
      },
      "outputs": [],
      "source": [
        "tacotron2 = tacotron2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ME0RQdUkuaY"
      },
      "source": [
        "### Load pretrained WaveGlow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBScP2cncalW",
        "outputId": "14cbc7ca-aded-46c1-a334-f8d2c7c3949a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WaveGlow(\n",
              "  (upsample): ConvTranspose1d(80, 80, kernel_size=(1024,), stride=(256,))\n",
              "  (WN): ModuleList(\n",
              "    (0): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (1): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (2): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (3): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (4): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (5): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (6): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (7): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (8): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (9): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (10): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (11): WN(\n",
              "      (in_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
              "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
              "      )\n",
              "      (res_skip_layers): ModuleList(\n",
              "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (cond_layers): ModuleList(\n",
              "        (0): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (1): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (2): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (3): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (4): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (5): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (6): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "        (7): Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
              "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "  )\n",
              "  (convinv): ModuleList(\n",
              "    (0): Invertible1x1Conv(\n",
              "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (1): Invertible1x1Conv(\n",
              "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (2): Invertible1x1Conv(\n",
              "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (3): Invertible1x1Conv(\n",
              "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (4): Invertible1x1Conv(\n",
              "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (5): Invertible1x1Conv(\n",
              "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (6): Invertible1x1Conv(\n",
              "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (7): Invertible1x1Conv(\n",
              "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (8): Invertible1x1Conv(\n",
              "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (9): Invertible1x1Conv(\n",
              "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (10): Invertible1x1Conv(\n",
              "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "    (11): Invertible1x1Conv(\n",
              "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ti8wOjr9chV4"
      },
      "outputs": [],
      "source": [
        "for name, param in waveglow.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pretrained r3d_18 (3D-ResNet) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi_syQ7j4rwR",
        "outputId": "765441d7-667c-4436-e219-4fdde3edb699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "r3d_18 = models.video.r3d_18(pretrained=False)\n",
        "encoder_model = r3d_18.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phjYsxPtH_MQ",
        "outputId": "ea134393-3cbc-486a-9e58-dd69aeb7d28f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VideoResNet(\n",
              "  (stem): BasicStem(\n",
              "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
              "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=148, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Model modifications\n",
        "encoder_model.fc = nn.Linear(in_features=512, out_features=148, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWt6cqot5tyD",
        "outputId": "68e79a6b-4702-4afb-f263-4fdd184d2ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name : stem.0.weight, parameter: torch.Size([64, 3, 3, 7, 7])\n",
            "name : stem.1.weight, parameter: torch.Size([64])\n",
            "name : stem.1.bias, parameter: torch.Size([64])\n",
            "name : layer1.0.conv1.0.weight, parameter: torch.Size([64, 64, 3, 3, 3])\n",
            "name : layer1.0.conv1.1.weight, parameter: torch.Size([64])\n",
            "name : layer1.0.conv1.1.bias, parameter: torch.Size([64])\n",
            "name : layer1.0.conv2.0.weight, parameter: torch.Size([64, 64, 3, 3, 3])\n",
            "name : layer1.0.conv2.1.weight, parameter: torch.Size([64])\n",
            "name : layer1.0.conv2.1.bias, parameter: torch.Size([64])\n",
            "name : layer1.1.conv1.0.weight, parameter: torch.Size([64, 64, 3, 3, 3])\n",
            "name : layer1.1.conv1.1.weight, parameter: torch.Size([64])\n",
            "name : layer1.1.conv1.1.bias, parameter: torch.Size([64])\n",
            "name : layer1.1.conv2.0.weight, parameter: torch.Size([64, 64, 3, 3, 3])\n",
            "name : layer1.1.conv2.1.weight, parameter: torch.Size([64])\n",
            "name : layer1.1.conv2.1.bias, parameter: torch.Size([64])\n",
            "name : layer2.0.conv1.0.weight, parameter: torch.Size([128, 64, 3, 3, 3])\n",
            "name : layer2.0.conv1.1.weight, parameter: torch.Size([128])\n",
            "name : layer2.0.conv1.1.bias, parameter: torch.Size([128])\n",
            "name : layer2.0.conv2.0.weight, parameter: torch.Size([128, 128, 3, 3, 3])\n",
            "name : layer2.0.conv2.1.weight, parameter: torch.Size([128])\n",
            "name : layer2.0.conv2.1.bias, parameter: torch.Size([128])\n",
            "name : layer2.0.downsample.0.weight, parameter: torch.Size([128, 64, 1, 1, 1])\n",
            "name : layer2.0.downsample.1.weight, parameter: torch.Size([128])\n",
            "name : layer2.0.downsample.1.bias, parameter: torch.Size([128])\n",
            "name : layer2.1.conv1.0.weight, parameter: torch.Size([128, 128, 3, 3, 3])\n",
            "name : layer2.1.conv1.1.weight, parameter: torch.Size([128])\n",
            "name : layer2.1.conv1.1.bias, parameter: torch.Size([128])\n",
            "name : layer2.1.conv2.0.weight, parameter: torch.Size([128, 128, 3, 3, 3])\n",
            "name : layer2.1.conv2.1.weight, parameter: torch.Size([128])\n",
            "name : layer2.1.conv2.1.bias, parameter: torch.Size([128])\n",
            "name : layer3.0.conv1.0.weight, parameter: torch.Size([256, 128, 3, 3, 3])\n",
            "name : layer3.0.conv1.1.weight, parameter: torch.Size([256])\n",
            "name : layer3.0.conv1.1.bias, parameter: torch.Size([256])\n",
            "name : layer3.0.conv2.0.weight, parameter: torch.Size([256, 256, 3, 3, 3])\n",
            "name : layer3.0.conv2.1.weight, parameter: torch.Size([256])\n",
            "name : layer3.0.conv2.1.bias, parameter: torch.Size([256])\n",
            "name : layer3.0.downsample.0.weight, parameter: torch.Size([256, 128, 1, 1, 1])\n",
            "name : layer3.0.downsample.1.weight, parameter: torch.Size([256])\n",
            "name : layer3.0.downsample.1.bias, parameter: torch.Size([256])\n",
            "name : layer3.1.conv1.0.weight, parameter: torch.Size([256, 256, 3, 3, 3])\n",
            "name : layer3.1.conv1.1.weight, parameter: torch.Size([256])\n",
            "name : layer3.1.conv1.1.bias, parameter: torch.Size([256])\n",
            "name : layer3.1.conv2.0.weight, parameter: torch.Size([256, 256, 3, 3, 3])\n",
            "name : layer3.1.conv2.1.weight, parameter: torch.Size([256])\n",
            "name : layer3.1.conv2.1.bias, parameter: torch.Size([256])\n",
            "name : layer4.0.conv1.0.weight, parameter: torch.Size([512, 256, 3, 3, 3])\n",
            "name : layer4.0.conv1.1.weight, parameter: torch.Size([512])\n",
            "name : layer4.0.conv1.1.bias, parameter: torch.Size([512])\n",
            "name : layer4.0.conv2.0.weight, parameter: torch.Size([512, 512, 3, 3, 3])\n",
            "name : layer4.0.conv2.1.weight, parameter: torch.Size([512])\n",
            "name : layer4.0.conv2.1.bias, parameter: torch.Size([512])\n",
            "name : layer4.0.downsample.0.weight, parameter: torch.Size([512, 256, 1, 1, 1])\n",
            "name : layer4.0.downsample.1.weight, parameter: torch.Size([512])\n",
            "name : layer4.0.downsample.1.bias, parameter: torch.Size([512])\n",
            "name : layer4.1.conv1.0.weight, parameter: torch.Size([512, 512, 3, 3, 3])\n",
            "name : layer4.1.conv1.1.weight, parameter: torch.Size([512])\n",
            "name : layer4.1.conv1.1.bias, parameter: torch.Size([512])\n",
            "name : layer4.1.conv2.0.weight, parameter: torch.Size([512, 512, 3, 3, 3])\n",
            "name : layer4.1.conv2.1.weight, parameter: torch.Size([512])\n",
            "name : layer4.1.conv2.1.bias, parameter: torch.Size([512])\n",
            "name : fc.weight, parameter: torch.Size([148, 512])\n",
            "name : fc.bias, parameter: torch.Size([148])\n"
          ]
        }
      ],
      "source": [
        "for name, param in encoder_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f\"name : {name}, parameter: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ec0Vnaek3-9"
      },
      "source": [
        "### Defining different Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Feqp3d-ijl-L"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
        "                     stride=strd, padding=padding, bias=bias)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n",
        "        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n",
        "        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n",
        "        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n",
        "        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n",
        "\n",
        "        if in_planes != out_planes:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.BatchNorm2d(in_planes),\n",
        "                nn.ReLU(True),\n",
        "                nn.Conv2d(in_planes, out_planes,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out1 = self.bn1(x)\n",
        "        out1 = F.relu(out1, True)\n",
        "        out1 = self.conv1(out1)\n",
        "\n",
        "        out2 = self.bn2(out1)\n",
        "        out2 = F.relu(out2, True)\n",
        "        out2 = self.conv2(out2)\n",
        "\n",
        "        out3 = self.bn3(out2)\n",
        "        out3 = F.relu(out3, True)\n",
        "        out3 = self.conv3(out3)\n",
        "\n",
        "        out3 = torch.cat((out1, out2, out3), 1)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(residual)\n",
        "\n",
        "        out3 += residual\n",
        "\n",
        "        return out3\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class HourGlass(nn.Module):\n",
        "    def __init__(self, num_modules, depth, num_features):\n",
        "        super(HourGlass, self).__init__()\n",
        "        self.num_modules = num_modules\n",
        "        self.depth = depth\n",
        "        self.features = num_features\n",
        "\n",
        "        self._generate_network(self.depth)\n",
        "\n",
        "    def _generate_network(self, level):\n",
        "        self.add_module('b1_' + str(level), ConvBlock(self.features, self.features))\n",
        "\n",
        "        self.add_module('b2_' + str(level), ConvBlock(self.features, self.features))\n",
        "\n",
        "        if level > 1:\n",
        "            self._generate_network(level - 1)\n",
        "        else:\n",
        "            self.add_module('b2_plus_' + str(level), ConvBlock(self.features, self.features))\n",
        "\n",
        "        self.add_module('b3_' + str(level), ConvBlock(self.features, self.features))\n",
        "\n",
        "    def _forward(self, level, inp):\n",
        "        # Upper branch\n",
        "        up1 = inp\n",
        "        up1 = self._modules['b1_' + str(level)](up1)\n",
        "\n",
        "        # Lower branch\n",
        "        low1 = F.avg_pool2d(inp, 2, stride=2)\n",
        "        low1 = self._modules['b2_' + str(level)](low1)\n",
        "\n",
        "        if level > 1:\n",
        "            low2 = self._forward(level - 1, low1)\n",
        "        else:\n",
        "            low2 = low1\n",
        "            low2 = self._modules['b2_plus_' + str(level)](low2)\n",
        "\n",
        "        low3 = low2\n",
        "        low3 = self._modules['b3_' + str(level)](low3)\n",
        "\n",
        "        up2 = F.interpolate(low3, scale_factor=2, mode='nearest')\n",
        "\n",
        "        return up1 + up2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(self.depth, x)\n",
        "\n",
        "\n",
        "class FAN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_modules=1):\n",
        "        super(FAN, self).__init__()\n",
        "        self.num_modules = num_modules\n",
        "\n",
        "        # Base part\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = ConvBlock(64, 128)\n",
        "        self.conv3 = ConvBlock(128, 128)\n",
        "        self.conv4 = ConvBlock(128, 256)\n",
        "\n",
        "        # Stacking part\n",
        "        for hg_module in range(self.num_modules):\n",
        "            self.add_module('m' + str(hg_module), HourGlass(1, 4, 256))\n",
        "            self.add_module('top_m_' + str(hg_module), ConvBlock(256, 256))\n",
        "            self.add_module('conv_last' + str(hg_module),\n",
        "                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
        "            self.add_module('bn_end' + str(hg_module), nn.BatchNorm2d(256))\n",
        "            self.add_module('l' + str(hg_module), nn.Conv2d(256,\n",
        "                                                            68, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "            if hg_module < self.num_modules - 1:\n",
        "                self.add_module(\n",
        "                    'bl' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
        "                self.add_module('al' + str(hg_module), nn.Conv2d(68,\n",
        "                                                                 256, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)), True)\n",
        "        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        previous = x\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(self.num_modules):\n",
        "            hg = self._modules['m' + str(i)](previous)\n",
        "\n",
        "            ll = hg\n",
        "            ll = self._modules['top_m_' + str(i)](ll)\n",
        "\n",
        "            ll = F.relu(self._modules['bn_end' + str(i)]\n",
        "                        (self._modules['conv_last' + str(i)](ll)), True)\n",
        "\n",
        "            # Predict heatmaps\n",
        "            tmp_out = self._modules['l' + str(i)](ll)\n",
        "            outputs.append(tmp_out)\n",
        "\n",
        "            if i < self.num_modules - 1:\n",
        "                ll = self._modules['bl' + str(i)](ll)\n",
        "                tmp_out_ = self._modules['al' + str(i)](tmp_out)\n",
        "                previous = previous + ll + tmp_out_\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zq5KVLzrmgll"
      },
      "outputs": [],
      "source": [
        "#@title ResNetDepth\n",
        "class ResNetDepth(nn.Module):\n",
        "    def __init__(self, block=Bottleneck, layers=[1, 1, 1, 1], num_classes=68): #36 tha\n",
        "        self.inplanes = 64\n",
        "        super(ResNetDepth, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3 * 75, 64, kernel_size=3, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DkTdmbyF03ir"
      },
      "outputs": [],
      "source": [
        "#@title My Encoder\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 225, out_channels = 6, kernel_size = (5,5))\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(13456, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 142)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -> n, 3, 32, 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
        "        x = x.view(-1, 13456)            # -> n, 400\n",
        "        x = F.relu(self.fc1(x))               # -> n, 120\n",
        "        x = F.relu(self.fc2(x))               # -> n, 84\n",
        "        x = self.fc3(x)                       # -> n, 10\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VjGN3l0BB5B",
        "outputId": "f33f8443-e839-46f9-c863-1639f3747071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name : conv1.weight, parameter: torch.Size([6, 225, 5, 5])\n",
            "name : conv1.bias, parameter: torch.Size([6])\n",
            "name : conv2.weight, parameter: torch.Size([16, 6, 5, 5])\n",
            "name : conv2.bias, parameter: torch.Size([16])\n",
            "name : fc1.weight, parameter: torch.Size([120, 13456])\n",
            "name : fc1.bias, parameter: torch.Size([120])\n",
            "name : fc2.weight, parameter: torch.Size([84, 120])\n",
            "name : fc2.bias, parameter: torch.Size([84])\n",
            "name : fc3.weight, parameter: torch.Size([142, 84])\n",
            "name : fc3.bias, parameter: torch.Size([142])\n"
          ]
        }
      ],
      "source": [
        "for name, param in Encoder().named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(f\"name : {name}, parameter: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8IVPCCbjl-d"
      },
      "source": [
        "### Image-Sequence data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WaCRnA2ljl-d"
      },
      "outputs": [],
      "source": [
        "seg_path = r\"/content/lec_01_segmented_frames\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zoOoLCnCqneg"
      },
      "outputs": [],
      "source": [
        "lower = 100\n",
        "upper = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "U2mP8vzDp1vB"
      },
      "outputs": [],
      "source": [
        "#@title Segment dataloading functions\n",
        "\n",
        "IMG_SIZE = 128\n",
        "\n",
        "def seg_sorter(seg: str):\n",
        "    return int(seg[4:])\n",
        "\n",
        "def load_segment(dir:str, start : int, end: int):\n",
        "    data_path = dir\n",
        "    data_arr = []\n",
        "    path = os.listdir(data_path)\n",
        "    path.sort(key=seg_sorter)\n",
        "    for file in path[start:end]:\n",
        "        seg_arr = []\n",
        "        print(f\"loaded : {file}\")\n",
        "        sub_path = os.path.join(data_path,file)\n",
        "        for img in os.listdir(sub_path):\n",
        "            img_path = os.path.join(data_path,file,img)\n",
        "            a = cv2.imread(img_path)\n",
        "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
        "            a = cv2.resize(a, (IMG_SIZE, IMG_SIZE)) \n",
        "            seg_arr.append(a)\n",
        "        data_arr.append(seg_arr)\n",
        "    nd_arr = np.array(data_arr)\n",
        "    return nd_arr\n",
        "\n",
        "\n",
        "def plot_segment(seg : np.ndarray):\n",
        "    fig = plt.figure(figsize=(15,13))\n",
        "    for i in range(75):  \n",
        "        ax = fig.add_subplot(8, 10, i+1)\n",
        "        ax.imshow(seg[i,:,:,:])\n",
        "        ax.set_title('frame:{y}'.format(y=i))\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkbqa_SSqXI6",
        "outputId": "33f41e34-42f4-4778-faba-fbee1b697ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded : seg_101\n",
            "loaded : seg_102\n",
            "loaded : seg_103\n",
            "loaded : seg_104\n",
            "loaded : seg_105\n",
            "loaded : seg_106\n",
            "loaded : seg_107\n",
            "loaded : seg_108\n",
            "loaded : seg_109\n",
            "loaded : seg_110\n",
            "loaded : seg_111\n",
            "loaded : seg_112\n",
            "loaded : seg_113\n",
            "loaded : seg_114\n",
            "loaded : seg_115\n",
            "loaded : seg_116\n",
            "loaded : seg_117\n",
            "loaded : seg_118\n",
            "loaded : seg_119\n",
            "loaded : seg_120\n",
            "loaded : seg_121\n",
            "loaded : seg_122\n",
            "loaded : seg_123\n",
            "loaded : seg_124\n",
            "loaded : seg_125\n",
            "loaded : seg_126\n",
            "loaded : seg_127\n",
            "loaded : seg_128\n",
            "loaded : seg_129\n",
            "loaded : seg_130\n",
            "loaded : seg_131\n",
            "loaded : seg_132\n",
            "loaded : seg_133\n",
            "loaded : seg_134\n",
            "loaded : seg_135\n",
            "loaded : seg_136\n",
            "loaded : seg_137\n",
            "loaded : seg_138\n",
            "loaded : seg_139\n",
            "loaded : seg_140\n",
            "loaded : seg_141\n",
            "loaded : seg_142\n",
            "loaded : seg_143\n",
            "loaded : seg_144\n",
            "loaded : seg_145\n",
            "loaded : seg_146\n",
            "loaded : seg_147\n",
            "loaded : seg_148\n",
            "loaded : seg_149\n",
            "loaded : seg_150\n",
            "loaded : seg_151\n",
            "loaded : seg_152\n",
            "loaded : seg_153\n",
            "loaded : seg_154\n",
            "loaded : seg_155\n",
            "loaded : seg_156\n",
            "loaded : seg_157\n",
            "loaded : seg_158\n",
            "loaded : seg_159\n",
            "loaded : seg_160\n",
            "loaded : seg_161\n",
            "loaded : seg_162\n",
            "loaded : seg_163\n",
            "loaded : seg_164\n",
            "loaded : seg_165\n",
            "loaded : seg_166\n",
            "loaded : seg_167\n",
            "loaded : seg_168\n",
            "loaded : seg_169\n",
            "loaded : seg_170\n",
            "loaded : seg_171\n",
            "loaded : seg_172\n",
            "loaded : seg_173\n",
            "loaded : seg_174\n",
            "loaded : seg_175\n",
            "loaded : seg_176\n",
            "loaded : seg_177\n",
            "loaded : seg_178\n",
            "loaded : seg_179\n",
            "loaded : seg_180\n",
            "loaded : seg_181\n",
            "loaded : seg_182\n",
            "loaded : seg_183\n",
            "loaded : seg_184\n",
            "loaded : seg_185\n",
            "loaded : seg_186\n",
            "loaded : seg_187\n",
            "loaded : seg_188\n",
            "loaded : seg_189\n",
            "loaded : seg_190\n",
            "loaded : seg_191\n",
            "loaded : seg_192\n",
            "loaded : seg_193\n",
            "loaded : seg_194\n",
            "loaded : seg_195\n",
            "loaded : seg_196\n",
            "loaded : seg_197\n",
            "loaded : seg_198\n",
            "loaded : seg_199\n",
            "loaded : seg_200\n"
          ]
        }
      ],
      "source": [
        "X = load_segment(seg_path, start = lower, end = upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI0zjq0GsDbD"
      },
      "source": [
        "### Audio data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "q70qNWKwudQ_"
      },
      "outputs": [],
      "source": [
        "audio_path = r\"/content/lec_01_audio_segmented\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "form",
        "id": "dLsSMNkisGxL"
      },
      "outputs": [],
      "source": [
        "#@title Audio loading function\n",
        "\n",
        "def audio_seg_sorter(seg: str):\n",
        "    splitted = seg.split('.')\n",
        "    return int(splitted[0][4:])\n",
        "\n",
        "def load_audio(dir:str, start : int, end: int):\n",
        "  data_arr = []\n",
        "  path = os.listdir(dir)\n",
        "  path.sort(key=audio_seg_sorter)\n",
        "  #print(path)\n",
        "  for file in path[start:end]:\n",
        "      seg_arr = []\n",
        "      print(file)\n",
        "      seg_file_temp = os.path.join(dir,file)\n",
        "      audio_seg_temp = librosa.load(seg_file_temp)\n",
        "      data_arr.append(audio_seg_temp)\n",
        "      print(seg_file_temp)\n",
        "  return data_arr\n",
        "\n",
        "def audio_to_mel_display(audio : np.ndarray):\n",
        "  spec = np.abs(librosa.stft(audio, hop_length=512))\n",
        "  spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
        "  print(spec.shape)\n",
        "  librosa.display.specshow(spec, sr=22050, x_axis='time', y_axis='log')\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "  plt.title('Spectrogram') \n",
        "\n",
        "def audio_to_mel_display_grid(audio_arr,number):\n",
        "  fig = plt.figure(figsize=(15,13))\n",
        "  for i in range(number):  \n",
        "      ax = fig.add_subplot(8, 10, i+1)\n",
        "      spec = np.abs(librosa.stft(audio_arr[i][0], hop_length=512))\n",
        "      spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
        "      librosa.display.specshow(spec, sr=22050, x_axis='time', y_axis='log')\n",
        "      #ax.colorbar(format='%+2.0f dB')\n",
        "      ax.set_title('seg:{y}'.format(y=i))\n",
        "      ax.axis('off')  \n",
        "\n",
        "def listen_audio(audio,sr):\n",
        "   ipd.Audio(audio, rate=sr)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n4EyQhAuF9O",
        "outputId": "388e6ff8-98bd-4e57-bd90-6c975f4fd50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seg_101.wav\n",
            "/content/lec_01_audio_segmented/seg_101.wav\n",
            "seg_102.wav\n",
            "/content/lec_01_audio_segmented/seg_102.wav\n",
            "seg_103.wav\n",
            "/content/lec_01_audio_segmented/seg_103.wav\n",
            "seg_104.wav\n",
            "/content/lec_01_audio_segmented/seg_104.wav\n",
            "seg_105.wav\n",
            "/content/lec_01_audio_segmented/seg_105.wav\n",
            "seg_106.wav\n",
            "/content/lec_01_audio_segmented/seg_106.wav\n",
            "seg_107.wav\n",
            "/content/lec_01_audio_segmented/seg_107.wav\n",
            "seg_108.wav\n",
            "/content/lec_01_audio_segmented/seg_108.wav\n",
            "seg_109.wav\n",
            "/content/lec_01_audio_segmented/seg_109.wav\n",
            "seg_110.wav\n",
            "/content/lec_01_audio_segmented/seg_110.wav\n",
            "seg_111.wav\n",
            "/content/lec_01_audio_segmented/seg_111.wav\n",
            "seg_112.wav\n",
            "/content/lec_01_audio_segmented/seg_112.wav\n",
            "seg_113.wav\n",
            "/content/lec_01_audio_segmented/seg_113.wav\n",
            "seg_114.wav\n",
            "/content/lec_01_audio_segmented/seg_114.wav\n",
            "seg_115.wav\n",
            "/content/lec_01_audio_segmented/seg_115.wav\n",
            "seg_116.wav\n",
            "/content/lec_01_audio_segmented/seg_116.wav\n",
            "seg_117.wav\n",
            "/content/lec_01_audio_segmented/seg_117.wav\n",
            "seg_118.wav\n",
            "/content/lec_01_audio_segmented/seg_118.wav\n",
            "seg_119.wav\n",
            "/content/lec_01_audio_segmented/seg_119.wav\n",
            "seg_120.wav\n",
            "/content/lec_01_audio_segmented/seg_120.wav\n",
            "seg_121.wav\n",
            "/content/lec_01_audio_segmented/seg_121.wav\n",
            "seg_122.wav\n",
            "/content/lec_01_audio_segmented/seg_122.wav\n",
            "seg_123.wav\n",
            "/content/lec_01_audio_segmented/seg_123.wav\n",
            "seg_124.wav\n",
            "/content/lec_01_audio_segmented/seg_124.wav\n",
            "seg_125.wav\n",
            "/content/lec_01_audio_segmented/seg_125.wav\n",
            "seg_126.wav\n",
            "/content/lec_01_audio_segmented/seg_126.wav\n",
            "seg_127.wav\n",
            "/content/lec_01_audio_segmented/seg_127.wav\n",
            "seg_128.wav\n",
            "/content/lec_01_audio_segmented/seg_128.wav\n",
            "seg_129.wav\n",
            "/content/lec_01_audio_segmented/seg_129.wav\n",
            "seg_130.wav\n",
            "/content/lec_01_audio_segmented/seg_130.wav\n",
            "seg_131.wav\n",
            "/content/lec_01_audio_segmented/seg_131.wav\n",
            "seg_132.wav\n",
            "/content/lec_01_audio_segmented/seg_132.wav\n",
            "seg_133.wav\n",
            "/content/lec_01_audio_segmented/seg_133.wav\n",
            "seg_134.wav\n",
            "/content/lec_01_audio_segmented/seg_134.wav\n",
            "seg_135.wav\n",
            "/content/lec_01_audio_segmented/seg_135.wav\n",
            "seg_136.wav\n",
            "/content/lec_01_audio_segmented/seg_136.wav\n",
            "seg_137.wav\n",
            "/content/lec_01_audio_segmented/seg_137.wav\n",
            "seg_138.wav\n",
            "/content/lec_01_audio_segmented/seg_138.wav\n",
            "seg_139.wav\n",
            "/content/lec_01_audio_segmented/seg_139.wav\n",
            "seg_140.wav\n",
            "/content/lec_01_audio_segmented/seg_140.wav\n",
            "seg_141.wav\n",
            "/content/lec_01_audio_segmented/seg_141.wav\n",
            "seg_142.wav\n",
            "/content/lec_01_audio_segmented/seg_142.wav\n",
            "seg_143.wav\n",
            "/content/lec_01_audio_segmented/seg_143.wav\n",
            "seg_144.wav\n",
            "/content/lec_01_audio_segmented/seg_144.wav\n",
            "seg_145.wav\n",
            "/content/lec_01_audio_segmented/seg_145.wav\n",
            "seg_146.wav\n",
            "/content/lec_01_audio_segmented/seg_146.wav\n",
            "seg_147.wav\n",
            "/content/lec_01_audio_segmented/seg_147.wav\n",
            "seg_148.wav\n",
            "/content/lec_01_audio_segmented/seg_148.wav\n",
            "seg_149.wav\n",
            "/content/lec_01_audio_segmented/seg_149.wav\n",
            "seg_150.wav\n",
            "/content/lec_01_audio_segmented/seg_150.wav\n",
            "seg_151.wav\n",
            "/content/lec_01_audio_segmented/seg_151.wav\n",
            "seg_152.wav\n",
            "/content/lec_01_audio_segmented/seg_152.wav\n",
            "seg_153.wav\n",
            "/content/lec_01_audio_segmented/seg_153.wav\n",
            "seg_154.wav\n",
            "/content/lec_01_audio_segmented/seg_154.wav\n",
            "seg_155.wav\n",
            "/content/lec_01_audio_segmented/seg_155.wav\n",
            "seg_156.wav\n",
            "/content/lec_01_audio_segmented/seg_156.wav\n",
            "seg_157.wav\n",
            "/content/lec_01_audio_segmented/seg_157.wav\n",
            "seg_158.wav\n",
            "/content/lec_01_audio_segmented/seg_158.wav\n",
            "seg_159.wav\n",
            "/content/lec_01_audio_segmented/seg_159.wav\n",
            "seg_160.wav\n",
            "/content/lec_01_audio_segmented/seg_160.wav\n",
            "seg_161.wav\n",
            "/content/lec_01_audio_segmented/seg_161.wav\n",
            "seg_162.wav\n",
            "/content/lec_01_audio_segmented/seg_162.wav\n",
            "seg_163.wav\n",
            "/content/lec_01_audio_segmented/seg_163.wav\n",
            "seg_164.wav\n",
            "/content/lec_01_audio_segmented/seg_164.wav\n",
            "seg_165.wav\n",
            "/content/lec_01_audio_segmented/seg_165.wav\n",
            "seg_166.wav\n",
            "/content/lec_01_audio_segmented/seg_166.wav\n",
            "seg_167.wav\n",
            "/content/lec_01_audio_segmented/seg_167.wav\n",
            "seg_168.wav\n",
            "/content/lec_01_audio_segmented/seg_168.wav\n",
            "seg_169.wav\n",
            "/content/lec_01_audio_segmented/seg_169.wav\n",
            "seg_170.wav\n",
            "/content/lec_01_audio_segmented/seg_170.wav\n",
            "seg_171.wav\n",
            "/content/lec_01_audio_segmented/seg_171.wav\n",
            "seg_172.wav\n",
            "/content/lec_01_audio_segmented/seg_172.wav\n",
            "seg_173.wav\n",
            "/content/lec_01_audio_segmented/seg_173.wav\n",
            "seg_174.wav\n",
            "/content/lec_01_audio_segmented/seg_174.wav\n",
            "seg_175.wav\n",
            "/content/lec_01_audio_segmented/seg_175.wav\n",
            "seg_176.wav\n",
            "/content/lec_01_audio_segmented/seg_176.wav\n",
            "seg_177.wav\n",
            "/content/lec_01_audio_segmented/seg_177.wav\n",
            "seg_178.wav\n",
            "/content/lec_01_audio_segmented/seg_178.wav\n",
            "seg_179.wav\n",
            "/content/lec_01_audio_segmented/seg_179.wav\n",
            "seg_180.wav\n",
            "/content/lec_01_audio_segmented/seg_180.wav\n",
            "seg_181.wav\n",
            "/content/lec_01_audio_segmented/seg_181.wav\n",
            "seg_182.wav\n",
            "/content/lec_01_audio_segmented/seg_182.wav\n",
            "seg_183.wav\n",
            "/content/lec_01_audio_segmented/seg_183.wav\n",
            "seg_184.wav\n",
            "/content/lec_01_audio_segmented/seg_184.wav\n",
            "seg_185.wav\n",
            "/content/lec_01_audio_segmented/seg_185.wav\n",
            "seg_186.wav\n",
            "/content/lec_01_audio_segmented/seg_186.wav\n",
            "seg_187.wav\n",
            "/content/lec_01_audio_segmented/seg_187.wav\n",
            "seg_188.wav\n",
            "/content/lec_01_audio_segmented/seg_188.wav\n",
            "seg_189.wav\n",
            "/content/lec_01_audio_segmented/seg_189.wav\n",
            "seg_190.wav\n",
            "/content/lec_01_audio_segmented/seg_190.wav\n",
            "seg_191.wav\n",
            "/content/lec_01_audio_segmented/seg_191.wav\n",
            "seg_192.wav\n",
            "/content/lec_01_audio_segmented/seg_192.wav\n",
            "seg_193.wav\n",
            "/content/lec_01_audio_segmented/seg_193.wav\n",
            "seg_194.wav\n",
            "/content/lec_01_audio_segmented/seg_194.wav\n",
            "seg_195.wav\n",
            "/content/lec_01_audio_segmented/seg_195.wav\n",
            "seg_196.wav\n",
            "/content/lec_01_audio_segmented/seg_196.wav\n",
            "seg_197.wav\n",
            "/content/lec_01_audio_segmented/seg_197.wav\n",
            "seg_198.wav\n",
            "/content/lec_01_audio_segmented/seg_198.wav\n",
            "seg_199.wav\n",
            "/content/lec_01_audio_segmented/seg_199.wav\n",
            "seg_200.wav\n",
            "/content/lec_01_audio_segmented/seg_200.wav\n"
          ]
        }
      ],
      "source": [
        "Y = load_audio(audio_path, start = lower, end = upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTZHs3-Wjl-i"
      },
      "source": [
        "### DataShuffling and Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KJ8xAfFiu5id"
      },
      "outputs": [],
      "source": [
        "#@title test-train split\n",
        "train_X = X[:960] # 70% \n",
        "train_Y = Y[:960] # 70% \n",
        "\n",
        "test_X = X[960:] # 30% \n",
        "test_Y = Y[960:] # 30% "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GjjQFFdkvmey"
      },
      "outputs": [],
      "source": [
        "train_len = len(train_X)\n",
        "train = []\n",
        "for i in range(train_len):\n",
        "  train.append((train_X[i],train_Y[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8PnIUcVsv7jT"
      },
      "outputs": [],
      "source": [
        "test_len = len(test_X)\n",
        "test = []\n",
        "for i in range(test_len):\n",
        "  test.append((test_X[i],test_Y[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DyagtZvyjl-y"
      },
      "outputs": [],
      "source": [
        "trainset = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EelgK7R6jl-0"
      },
      "source": [
        "### All Encoder Models Instantiated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter, Optimizer and Loss Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1fezIYgVjl-z"
      },
      "outputs": [],
      "source": [
        "#@title Hyper-Parameters\n",
        "num_epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "43CJNRzbjl-2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "okmSlGphjl-3",
        "outputId": "cfa13a8f-795c-40ff-f10f-158724b54b8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 75, 128, 128, 3]) torch.Size([1, 66150])\n",
            "torch.Size([1, 3, 75, 128, 128]) torch.float32\n",
            "torch.Size([1, 148])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-88520d39293e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtacotron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/model.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0membedded_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         mel_outputs, gate_outputs, alignments, mel_lengths = self.decoder.infer(\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ],
      "source": [
        "from torch.autograd import forward_ad\n",
        "n_total_steps = len(trainset)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (seg, aud) in enumerate(trainset):\n",
        "        mel = np.asarray(aud)\n",
        "        print(seg.shape,mel[0].shape)\n",
        "        seg = seg.reshape((1,3,75,128,128))\n",
        "        seg = seg.float()\n",
        "        print(seg.shape,seg.dtype)\n",
        "\n",
        "        seg = seg.to(device)\n",
        "        aud = aud[0].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        encoded = model(seg)\n",
        "\n",
        "        print(encoded.shape,encoded.dtype)\n",
        "        #print(encoded)\n",
        "\n",
        "        length = encoded.T.shape\n",
        "        length = torch.tensor(length)\n",
        "        length = length.to(device)\n",
        "\n",
        "        mel, _ , _ = tacotron2.infer(encoded.T,length)\n",
        "      \n",
        "        print(mel.shape)\n",
        "        audio = waveglow.infer(mel)\n",
        "        loss = criterion(audio, aud)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1s_qv3Zjl-4"
      },
      "source": [
        "### Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qyjI_4njl-4"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for seg, aud in testset:\n",
        "        seg = seg.to(device)\n",
        "        aud = aud.to(device)\n",
        "        outputs = model(seg)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += mel.size(0)\n",
        "        n_correct += (predicted == mel).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJC6i8Zjl-5"
      },
      "source": [
        "### Save Model to \\<path\\>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti3BFCPxjl-5"
      },
      "outputs": [],
      "source": [
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "df489fb1e130da9f6ba5d7b964fc8c0a95ac36686e5462b4af8e73863edb035e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
